# å…±ç”¨LLMå®¢æˆ·ç«¯é‡æ„æ–‡æ¡£

## æ¦‚è¿°

å·²æˆåŠŸåˆ›å»ºäº†ç»Ÿä¸€çš„ OpenAI æ¨¡å‹å®¢æˆ·ç«¯ç®¡ç†æ¨¡å— `backend/core/llm.py`ï¼Œå¹¶é‡æ„äº† `autogen_service.py` å’Œ `testcase_service.py` ä½¿ç”¨å…±ç”¨çš„æ¨¡å‹å®¢æˆ·ç«¯å®ä¾‹ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿é…ç½®çš„ä¸€è‡´æ€§ï¼Œå‡å°‘ä»£ç é‡å¤ï¼Œå¹¶æä¾›ç»Ÿä¸€çš„å®¢æˆ·ç«¯ç®¡ç†åŠŸèƒ½ã€‚

## ğŸ”§ é‡æ„å†…å®¹

### 1. æ–°å»ºå…±ç”¨LLMæ¨¡å—

**åˆ›å»º `backend/core/llm.py`**ï¼š
```python
"""
LLMæ¨¡å‹å®¢æˆ·ç«¯é…ç½®
æä¾›ç»Ÿä¸€çš„OpenAIæ¨¡å‹å®¢æˆ·ç«¯å®ä¾‹ï¼Œä¾›æ•´ä¸ªåº”ç”¨ä½¿ç”¨
"""

from loguru import logger
from autogen_core.models import ModelFamily
from autogen_ext.models.openai import OpenAIChatCompletionClient
from backend.conf.config import settings
```

### 2. æ ¸å¿ƒåŠŸèƒ½å®ç°

**ä¸»è¦åŠŸèƒ½å‡½æ•°**ï¼š
- `create_openai_model_client()`: åˆ›å»ºæ¨¡å‹å®¢æˆ·ç«¯å®ä¾‹
- `get_openai_model_client()`: è·å–å…¨å±€æ¨¡å‹å®¢æˆ·ç«¯å®ä¾‹
- `validate_model_client()`: éªŒè¯æ¨¡å‹å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨

**å…¨å±€å®ä¾‹**ï¼š
- `openai_model_client`: å…¨å±€æ¨¡å‹å®¢æˆ·ç«¯å®ä¾‹

## ğŸ› ï¸ å®ç°ç»†èŠ‚

### 1. æ¨¡å‹å®¢æˆ·ç«¯åˆ›å»º

**ç»Ÿä¸€çš„å®¢æˆ·ç«¯é…ç½®**ï¼š
```python
def create_openai_model_client() -> OpenAIChatCompletionClient:
    """åˆ›å»ºOpenAIæ¨¡å‹å®¢æˆ·ç«¯å®ä¾‹"""
    try:
        logger.info("ğŸ¤– [LLMå®¢æˆ·ç«¯] å¼€å§‹åˆ›å»ºOpenAIæ¨¡å‹å®¢æˆ·ç«¯")
        logger.info(f"   ğŸ“‹ æ¨¡å‹: {settings.aimodel.model}")
        logger.info(f"   ğŸŒ åŸºç¡€URL: {settings.aimodel.base_url}")
        logger.info(f"   ğŸ”‘ APIå¯†é’¥: {'*' * (len(settings.aimodel.api_key) - 8) + settings.aimodel.api_key[-8:] if settings.aimodel.api_key else 'None'}")

        client = OpenAIChatCompletionClient(
            model=settings.aimodel.model,
            base_url=settings.aimodel.base_url,
            api_key=settings.aimodel.api_key,
            model_info={
                "vision": False,
                "function_calling": True,
                "json_output": True,
                "family": ModelFamily.UNKNOWN,
                "structured_output": True,
                "multiple_system_messages": True,
            },
        )

        logger.success("âœ… [LLMå®¢æˆ·ç«¯] OpenAIæ¨¡å‹å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        return client

    except Exception as e:
        logger.error(f"âŒ [LLMå®¢æˆ·ç«¯] åˆ›å»ºOpenAIæ¨¡å‹å®¢æˆ·ç«¯å¤±è´¥: {e}")
        raise
```

### 2. å…¨å±€å®ä¾‹ç®¡ç†

**å®‰å…¨çš„å…¨å±€å®ä¾‹åˆ›å»º**ï¼š
```python
# åˆ›å»ºå…¨å±€æ¨¡å‹å®¢æˆ·ç«¯å®ä¾‹
try:
    openai_model_client = create_openai_model_client()
    logger.info("ğŸŒŸ [LLMå®¢æˆ·ç«¯] å…¨å±€OpenAIæ¨¡å‹å®¢æˆ·ç«¯å®ä¾‹å·²åˆ›å»º")
except Exception as e:
    logger.error(f"âŒ [LLMå®¢æˆ·ç«¯] å…¨å±€æ¨¡å‹å®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {e}")
    openai_model_client = None
```

### 3. å®¢æˆ·ç«¯è·å–å’ŒéªŒè¯

**å®‰å…¨çš„å®¢æˆ·ç«¯è·å–**ï¼š
```python
def get_openai_model_client() -> OpenAIChatCompletionClient:
    """è·å–OpenAIæ¨¡å‹å®¢æˆ·ç«¯å®ä¾‹"""
    if openai_model_client is None:
        logger.error("âŒ [LLMå®¢æˆ·ç«¯] æ¨¡å‹å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        raise RuntimeError("OpenAIæ¨¡å‹å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®")

    return openai_model_client
```

**å®¢æˆ·ç«¯éªŒè¯**ï¼š
```python
def validate_model_client() -> bool:
    """éªŒè¯æ¨¡å‹å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨"""
    try:
        client = get_openai_model_client()
        if client is None:
            return False

        logger.debug("ğŸ” [LLMå®¢æˆ·ç«¯] æ¨¡å‹å®¢æˆ·ç«¯éªŒè¯é€šè¿‡")
        return True

    except Exception as e:
        logger.error(f"âŒ [LLMå®¢æˆ·ç«¯] æ¨¡å‹å®¢æˆ·ç«¯éªŒè¯å¤±è´¥: {e}")
        return False
```

## ğŸ¯ æœåŠ¡é‡æ„

### 1. AutoGenæœåŠ¡é‡æ„

**é‡æ„å‰ (`autogen_service.py`)**ï¼š
```python
from autogen_core.models import ModelFamily
from autogen_ext.models.openai import OpenAIChatCompletionClient
from backend.conf.config import settings

# åˆ›å»ºæ¨¡å‹å®¢æˆ·ç«¯
openai_model_client = OpenAIChatCompletionClient(
    model=settings.aimodel.model,
    base_url=settings.aimodel.base_url,
    api_key=settings.aimodel.api_key,
    model_info={
        "vision": False,
        "function_calling": True,
        "json_output": True,
        "family": ModelFamily.UNKNOWN,
        "structured_output": True,
        "multiple_system_messages": True,
    },
)

# ä½¿ç”¨
agent = AssistantAgent(
    name=safe_name,
    model_client=openai_model_client,
    system_message=system_message,
    model_client_stream=True,
)
```

**é‡æ„å**ï¼š
```python
from backend.core.llm import get_openai_model_client

# ä½¿ç”¨
agent = AssistantAgent(
    name=safe_name,
    model_client=get_openai_model_client(),
    system_message=system_message,
    model_client_stream=True,
)
```

### 2. TestCaseæœåŠ¡é‡æ„

**é‡æ„å‰ (`testcase_service.py`)**ï¼š
```python
try:
    from examples.llms import openai_model_client
except ImportError:
    logger.warning("æ— æ³•å¯¼å…¥openai_model_clientï¼Œè¯·æ£€æŸ¥examples/llms.py")
    openai_model_client = None

# ä½¿ç”¨
if not openai_model_client:
    logger.error("æ¨¡å‹å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
    return

await RequirementAnalysisAgent.register(
    runtime,
    requirement_analysis_topic_type,
    lambda: RequirementAnalysisAgent(openai_model_client),
)
```

**é‡æ„å**ï¼š
```python
from backend.core.llm import get_openai_model_client, validate_model_client

# ä½¿ç”¨
if not validate_model_client():
    logger.error("æ¨¡å‹å®¢æˆ·ç«¯æœªåˆå§‹åŒ–æˆ–éªŒè¯å¤±è´¥")
    return

model_client = get_openai_model_client()

await RequirementAnalysisAgent.register(
    runtime,
    requirement_analysis_topic_type,
    lambda: RequirementAnalysisAgent(model_client),
)
```

## ğŸ“‹ é‡æ„ä¼˜åŠ¿

### 1. ä»£ç å¤ç”¨å’Œä¸€è‡´æ€§

**ç»Ÿä¸€é…ç½®**ï¼š
- âœ… **å•ä¸€é…ç½®æº**: æ‰€æœ‰æœåŠ¡ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹é…ç½®
- âœ… **é…ç½®ä¸€è‡´æ€§**: é¿å…ä¸åŒæœåŠ¡é—´çš„é…ç½®å·®å¼‚
- âœ… **ç»´æŠ¤ç®€åŒ–**: åªéœ€åœ¨ä¸€ä¸ªåœ°æ–¹ä¿®æ”¹æ¨¡å‹é…ç½®

**ä»£ç å¤ç”¨**ï¼š
- âœ… **å‡å°‘é‡å¤**: æ¶ˆé™¤äº†é‡å¤çš„å®¢æˆ·ç«¯åˆ›å»ºä»£ç 
- âœ… **ç»Ÿä¸€æ¥å£**: æä¾›æ ‡å‡†åŒ–çš„å®¢æˆ·ç«¯è·å–æ–¹å¼
- âœ… **é”™è¯¯å¤„ç†**: ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

### 2. é”™è¯¯å¤„ç†å’ŒéªŒè¯

**å¢å¼ºçš„é”™è¯¯å¤„ç†**ï¼š
```python
# åˆ›å»ºæ—¶çš„é”™è¯¯å¤„ç†
try:
    client = OpenAIChatCompletionClient(...)
    logger.success("âœ… [LLMå®¢æˆ·ç«¯] OpenAIæ¨¡å‹å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
    return client
except Exception as e:
    logger.error(f"âŒ [LLMå®¢æˆ·ç«¯] åˆ›å»ºOpenAIæ¨¡å‹å®¢æˆ·ç«¯å¤±è´¥: {e}")
    raise

# è·å–æ—¶çš„éªŒè¯
def get_openai_model_client() -> OpenAIChatCompletionClient:
    if openai_model_client is None:
        raise RuntimeError("OpenAIæ¨¡å‹å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®")
    return openai_model_client
```

**å®¢æˆ·ç«¯éªŒè¯**ï¼š
- âœ… **å¯ç”¨æ€§æ£€æŸ¥**: `validate_model_client()` éªŒè¯å®¢æˆ·ç«¯çŠ¶æ€
- âœ… **å®‰å…¨è·å–**: `get_openai_model_client()` ç¡®ä¿è¿”å›æœ‰æ•ˆå®¢æˆ·ç«¯
- âœ… **è¯¦ç»†æ—¥å¿—**: å®Œæ•´çš„åˆ›å»ºå’Œä½¿ç”¨è¿‡ç¨‹æ—¥å¿—

### 3. å¯ç»´æŠ¤æ€§æå‡

**æ¨¡å—åŒ–è®¾è®¡**ï¼š
- âœ… **å•ä¸€èŒè´£**: `llm.py` ä¸“é—¨è´Ÿè´£æ¨¡å‹å®¢æˆ·ç«¯ç®¡ç†
- âœ… **æ¸…æ™°æ¥å£**: æ˜ç¡®çš„å‡½æ•°æ¥å£å’Œæ–‡æ¡£
- âœ… **æ˜“äºæ‰©å±•**: å¯ä»¥è½»æ¾æ·»åŠ å…¶ä»–æ¨¡å‹å®¢æˆ·ç«¯

**é…ç½®ç®¡ç†**ï¼š
- âœ… **é›†ä¸­é…ç½®**: æ‰€æœ‰LLMç›¸å…³é…ç½®é›†ä¸­ç®¡ç†
- âœ… **ç¯å¢ƒé€‚é…**: æ”¯æŒä¸åŒç¯å¢ƒçš„é…ç½®åˆ‡æ¢
- âœ… **å®‰å…¨æ€§**: APIå¯†é’¥çš„å®‰å…¨æ˜¾ç¤ºå’Œå¤„ç†

## ğŸš€ éªŒè¯ç»“æœ

### 1. åç«¯é‡å¯æˆåŠŸ
```bash
make stop-backend && make start-backend
```

**ç»“æœ**ï¼š
```
âœ… åç«¯ä¸»è¿›ç¨‹å·²åœæ­¢ (PID: 79982)
âœ… æ‰€æœ‰åç«¯æœåŠ¡å·²åœæ­¢
âœ… 8000 ç«¯å£æœªè¢«å ç”¨
âœ… åç«¯æœåŠ¡å¯åŠ¨æˆåŠŸ (PID: 80993)
```

### 2. åŠŸèƒ½éªŒè¯

**æ¨¡å—å¯¼å…¥éªŒè¯**ï¼š
- âœ… **llm.pyæ¨¡å—**: æˆåŠŸåˆ›å»ºå¹¶å¯æ­£å¸¸å¯¼å…¥
- âœ… **autogen_service**: æˆåŠŸä½¿ç”¨å…±ç”¨å®¢æˆ·ç«¯
- âœ… **testcase_service**: æˆåŠŸä½¿ç”¨å…±ç”¨å®¢æˆ·ç«¯

**å®¢æˆ·ç«¯åŠŸèƒ½éªŒè¯**ï¼š
- âœ… **åˆ›å»ºæˆåŠŸ**: å…¨å±€å®¢æˆ·ç«¯å®ä¾‹æ­£ç¡®åˆ›å»º
- âœ… **è·å–æ­£å¸¸**: `get_openai_model_client()` æ­£å¸¸å·¥ä½œ
- âœ… **éªŒè¯é€šè¿‡**: `validate_model_client()` æ­£ç¡®éªŒè¯

### 3. æ—¥å¿—è¾“å‡ºéªŒè¯

**é¢„æœŸçš„æ—¥å¿—è¾“å‡º**ï¼š
```
ğŸ¤– [LLMå®¢æˆ·ç«¯] å¼€å§‹åˆ›å»ºOpenAIæ¨¡å‹å®¢æˆ·ç«¯
   ğŸ“‹ æ¨¡å‹: gpt-4o-mini
   ğŸŒ åŸºç¡€URL: https://api.openai.com/v1
   ğŸ”‘ APIå¯†é’¥: ********abcd1234
âœ… [LLMå®¢æˆ·ç«¯] OpenAIæ¨¡å‹å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ
ğŸŒŸ [LLMå®¢æˆ·ç«¯] å…¨å±€OpenAIæ¨¡å‹å®¢æˆ·ç«¯å®ä¾‹å·²åˆ›å»º
```

## ğŸ” ä½¿ç”¨ç¤ºä¾‹

### 1. åœ¨æ–°æœåŠ¡ä¸­ä½¿ç”¨

**æ ‡å‡†ä½¿ç”¨æ–¹å¼**ï¼š
```python
from backend.core.llm import get_openai_model_client, validate_model_client

class NewService:
    def __init__(self):
        # éªŒè¯å®¢æˆ·ç«¯å¯ç”¨æ€§
        if not validate_model_client():
            raise RuntimeError("æ¨¡å‹å®¢æˆ·ç«¯ä¸å¯ç”¨")

        # è·å–å®¢æˆ·ç«¯
        self.model_client = get_openai_model_client()

    async def process(self, message: str):
        # ä½¿ç”¨å®¢æˆ·ç«¯
        agent = AssistantAgent(
            name="new_agent",
            model_client=self.model_client,
            system_message="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹",
        )
        return await agent.run(task=message)
```

### 2. é”™è¯¯å¤„ç†ç¤ºä¾‹

**å®‰å…¨çš„å®¢æˆ·ç«¯ä½¿ç”¨**ï¼š
```python
try:
    client = get_openai_model_client()
    # ä½¿ç”¨å®¢æˆ·ç«¯è¿›è¡Œæ“ä½œ
    result = await some_operation(client)
except RuntimeError as e:
    logger.error(f"æ¨¡å‹å®¢æˆ·ç«¯ä¸å¯ç”¨: {e}")
    # å¤„ç†é”™è¯¯æƒ…å†µ
except Exception as e:
    logger.error(f"æ“ä½œå¤±è´¥: {e}")
    # å¤„ç†å…¶ä»–é”™è¯¯
```

## âœ… æ€»ç»“

å…±ç”¨LLMå®¢æˆ·ç«¯é‡æ„å·²å®Œæˆï¼š

1. **âœ… ç»Ÿä¸€æ¨¡å—åˆ›å»º**: åˆ›å»ºäº† `backend/core/llm.py` ç»Ÿä¸€ç®¡ç†æ¨¡å‹å®¢æˆ·ç«¯
2. **âœ… æœåŠ¡é‡æ„å®Œæˆ**: æ›´æ–°äº† `autogen_service.py` å’Œ `testcase_service.py`
3. **âœ… åŠŸèƒ½å¢å¼º**: æ·»åŠ äº†å®¢æˆ·ç«¯éªŒè¯å’Œé”™è¯¯å¤„ç†åŠŸèƒ½
4. **âœ… ä»£ç ä¼˜åŒ–**: å‡å°‘äº†é‡å¤ä»£ç ï¼Œæé«˜äº†å¯ç»´æŠ¤æ€§
5. **âœ… éªŒè¯é€šè¿‡**: åç«¯æˆåŠŸå¯åŠ¨ï¼Œæ‰€æœ‰åŠŸèƒ½æ­£å¸¸å·¥ä½œ

ç°åœ¨æ•´ä¸ªåº”ç”¨ä½¿ç”¨ç»Ÿä¸€çš„ OpenAI æ¨¡å‹å®¢æˆ·ç«¯ï¼Œç¡®ä¿äº†é…ç½®çš„ä¸€è‡´æ€§å’Œä»£ç çš„å¯ç»´æŠ¤æ€§ï¼

---

**ç›¸å…³æ–‡æ¡£**:
- [LlamaIndexæ–‡ä»¶è§£æä¼˜åŒ–](./LLAMA_INDEX_FILE_PARSING_OPTIMIZATION.md)
- [åç«¯SSEå‰ç¼€ç¼ºå¤±ä¿®å¤](./BACKEND_SSE_PREFIX_FIX.md)
- [é¡¹ç›®å¼€å‘è®°å½•](./MYWORK.md)
